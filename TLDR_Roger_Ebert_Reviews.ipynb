{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN0W-cohbj0H",
        "outputId": "ffaae358-c7d7-427c-dfd8-66eb0a9849be"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# parse HTML\n",
        "def parseHTML(page):\n",
        "  return BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "def getText(soup):\n",
        "  return soup.find_all('section', { 'class', 'page-content--block_editor-content js--reframe' })\n",
        "\n",
        "def tokenizeSentences(fullText):\n",
        "  return sent_tokenize(fullText)\n",
        "\n",
        "def tokenizeWords(fullText):\n",
        "  return word_tokenize(fullText.lower())\n",
        "\n",
        "\n",
        "def removeStopWords(words):\n",
        "  # builds a set of stop words\n",
        "  stop_words = set(stopwords.words('english') + list(punctuation) + ['“', '’', '”', '`', \"'\", '\\\\'])\n",
        "  filteredWords = []\n",
        "  # removes all stop words and punctuation from input\n",
        "  for word in words:\n",
        "    if word not in stop_words:\n",
        "      filteredWords.append(word)\n",
        "  return filteredWords\n",
        "\n",
        "def concatenateText(foundSections):\n",
        "  fullText = \"\"\n",
        "  # concatenate the section texts together\n",
        "  for section in foundSections:\n",
        "    # removes all special unicode characters\n",
        "    fullText += unicodedata.normalize('NFKD', section.text)\n",
        "  return fullText\n",
        "\n",
        "\n",
        "# opens up HTML of a webpage\n",
        "# the url can be changed to any desired movie review url from rogerebert.com\n",
        "page = urllib.request.urlopen(\"https://www.rogerebert.com/reviews/indiana-jones-and-the-dial-of-destiny-movie-review-2023\").read().decode(\"utf-8\") \\\n",
        "\n",
        "soup = parseHTML(page)\n",
        "\n",
        "# Gets the text from section tags\n",
        "foundSections = getText(soup)\n",
        "\n",
        "fullText = concatenateText(foundSections)\n",
        "\n",
        "\n",
        "# generates an array of sentences from fullText\n",
        "sentences = tokenizeSentences(fullText)\n",
        "\n",
        "# tokenize the words\n",
        "words = tokenizeWords(fullText)\n",
        "\n",
        "filteredWords = removeStopWords(words)\n",
        "\n",
        "# gives frequency distribution of words\n",
        "freq = FreqDist(filteredWords)\n",
        "\n",
        "# get top 10 frequent words in freq\n",
        "nlargest(10, freq, key = freq.get)\n",
        "\n",
        "# ranks the sentences based on frequency of words in the total\n",
        "ranking = defaultdict(int)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for w in word_tokenize(sentence.lower()):\n",
        "    if w in freq:\n",
        "      ranking[i] += freq[w]\n",
        "\n",
        "sentenceIndexes = nlargest(4, ranking, key = ranking.get)\n",
        "[sentences[j] for j in sorted(sentenceIndexes)]\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
